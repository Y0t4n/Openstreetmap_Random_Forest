{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2c0941",
   "metadata": {},
   "source": [
    "This notebook file contains all the code used for the experiments that were done on random forest. Data and results can be found in the offically used data folder. This file is a cleaned up version of the file the experiments were run in. I kept that file accesible in the legacy code folder as \"randomforest_legacy.ipynb\", but this file contains all the useful code with none of the errant test functions and print statements. I also added more comments to this file for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b339e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrosm.data import sources\n",
    "import pyrosm\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "import pickle\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# For below import to work move convert_report2excel from support files to the same directory as this file\n",
    "from convert_report2excel import convert_report2excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_lists(pois, n: int) -> tuple[list[str], dict[str:int]]:\n",
    "    \"\"\"\n",
    "    Returns all_good_tags, a list of the tags that appear more than n\n",
    "    Returns tag2idx, a dictionary linking every allowed tag to an index number\n",
    "    \"\"\"\n",
    "\n",
    "    # This function is also used and introduced in our data notebook\n",
    "    # In this notebook we don't use the full functionality, we just need the tag2idx for our vectors\n",
    "    # For a better breakdown of this function, check the data notebook\n",
    "\n",
    "    tag_freq = defaultdict(int)\n",
    "    \n",
    "    for idx, row in pois.iterrows(): \n",
    "        tags = row.get(\"tags\", {})\n",
    "        \n",
    "        if isinstance(tags, dict) and tags:\n",
    "            for tag_key in tags:\n",
    "                tag_freq[tag_key] += 1\n",
    "\n",
    "    all_tags = list(tag_freq.keys())\n",
    "    all_good_tags = [tag for tag in tag_freq if tag_freq[tag] > n]\n",
    "    tag2idx = {tag: i for i, tag in enumerate(all_good_tags)}\n",
    "    idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
    "\n",
    "    tag_freq = dict(sorted(tag_freq.items(), key = lambda x: x[1], reverse = True))\n",
    "    print(f\"All tags sorted by frequency: {tag_freq}\")\n",
    "    print(f\" All allowed tags: {all_good_tags}\")\n",
    "    print(f\"Len all tags: {len(all_tags)}, Len good tags: {len(all_good_tags)}\")\n",
    "\n",
    "    # We only return these two objects\n",
    "    # During testing and in other versions I have returned tag_freq or idx2tag but that wasn't needed for the experiments\n",
    "    return (all_good_tags, tag2idx)    \n",
    "\n",
    "def tags_to_vec(tag_dict, tag2idx):\n",
    "    vector = np.zeros(len(tag2idx), dtype= np.float32)\n",
    "    if isinstance(tag_dict, dict) or isinstance(tag_dict, list):\n",
    "        for tag in tag_dict:\n",
    "            if tag in tag2idx:\n",
    "                vector[tag2idx[tag]] = 1\n",
    "    return vector\n",
    "\n",
    "def tags_to_vec_singular(solo_tag, tag2idx):\n",
    "    vector = np.zeros(len(tag2idx), dtype= np.float32)\n",
    "    if solo_tag in tag2idx:\n",
    "        vector[tag2idx[solo_tag]] = 1\n",
    "    return vector\n",
    "\n",
    "def vector_pois(pois, tag2idx, n):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for tag in pois[\"tags\"]:\n",
    "        if not isinstance(tag, dict) or len(tag) < n: # set the len to 1 will include empty {}\n",
    "            continue\n",
    "\n",
    "        tag_keys = list(tag.keys())\n",
    "\n",
    "        np.random.shuffle(tag_keys)\n",
    "        \n",
    "        feature_idx = len(tag_keys) // 2\n",
    "\n",
    "        input_tags = {k: tag[k] for k in tag_keys[:feature_idx] + tag_keys[feature_idx + 1:]}\n",
    "        output_tags = {tag_keys[feature_idx] : tag[tag_keys[feature_idx]]}\n",
    "\n",
    "        X.append(tags_to_vec(input_tags, tag2idx))\n",
    "        y.append(tags_to_vec(output_tags, tag2idx))\n",
    "\n",
    "    X = np.stack(X)\n",
    "    y = np.stack(y)\n",
    "    \n",
    "    print(\"X shape:\", X.shape)\n",
    "\n",
    "    print(\"y_shape\", y.shape)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def vector_pois_test(poisX, poisy, tag2idx, n):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for tag in poisX:\n",
    "        \n",
    "        if len(tag) < n: # set the len to 1 will include empty {}\n",
    "            print(tag)\n",
    "            continue\n",
    "        X.append(tags_to_vec(tag, tag2idx))\n",
    "    for tag in poisy:\n",
    "        y.append(tags_to_vec_singular(tag, tag2idx))\n",
    "\n",
    "    \n",
    "    X = np.stack(X)\n",
    "    y = np.stack(y)\n",
    "    print(\"X_shape:\", X.shape)\n",
    "\n",
    "    print(\"y_shape\", y.shape)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the training set to create the tree\n",
    "with open('trainingset', 'rb') as fp:\n",
    "    pois = pickle.load(fp)\n",
    "# And the test set for querying and answers for checking\n",
    "with open('testset_questions', 'rb') as fp:\n",
    "    test_pois_X = pickle.load(fp)\n",
    "with open('testset_answer', 'rb') as fp:\n",
    "    test_pois_y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_good_tags = 10 # Frequency of tags for them to be allowed\n",
    "n_per_instance = 1 # Amount of tags an instance needs to have to be part of the data\n",
    "                   # If set to 1 it will include empty cells\n",
    "\n",
    "\n",
    "good_tags, tag2idx = create_tag_lists(pois, n_good_tags)\n",
    "\n",
    "X, Y = vector_pois(pois, tag2idx, n_per_instance)\n",
    "X_test, y_test = vector_pois_test(test_pois_X, test_pois_y, tag2idx, 1)\n",
    "\n",
    "\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model = False\n",
    "if check_model:\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Define search space\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    # Wrap in RandomizedSearchCV\n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,  # Try 20 random combinations\n",
    "        cv=3,       # 3-fold cross-validation\n",
    "        scoring='f1_weighted',  # Or accuracy, or a custom metric\n",
    "        n_jobs=-1,  # Use all cores\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model after search\n",
    "    best_model = search.best_estimator_\n",
    "    print(best_model)\n",
    "\n",
    "if not check_model:\n",
    "    best_model = RandomForestClassifier(max_features='log2', n_estimators=300, max_depth=None) # Thepreviousbestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_thresholds(y_true, y_probs, thresholds=np.arange(0.1, 0.9, 9)):\n",
    "    best_thresholds = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        best_f1 = 0\n",
    "        best_thresh = 0.5\n",
    "        for thresh in thresholds:\n",
    "            preds = (y_probs[:, i] >= thresh).astype(int)\n",
    "            f1 = f1_score(y_true[:, i], preds, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_thresh = thresh\n",
    "        best_thresholds.append(best_thresh)\n",
    "    return np.array(best_thresholds)\n",
    "\n",
    "Y_probs_val = best_model.predict_proba(X_val)      \n",
    "prob_matrix_val = np.array([\n",
    "    probs[:, 1] if probs.shape[1] > 1 else np.zeros(probs.shape[0])\n",
    "    for probs in Y_probs_val]).T\n",
    "\n",
    "thresholds = find_best_thresholds(y_val, prob_matrix_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_probs_test = best_model.predict_proba(X_test)\n",
    "\n",
    "prob_matrix_test = np.array([\n",
    "     probs[:, 1] if probs.shape[1] > 1 else np.zeros(probs.shape[0])\n",
    "     for probs in Y_probs_test]).T\n",
    "\n",
    "Y_preds = (prob_matrix_test >= thresholds).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, Y_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = Workbook()\n",
    "workbook.remove(workbook.active) # Delete default sheet.\n",
    "\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    Y_preds,\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "workbook = convert_report2excel(\n",
    "    workbook=workbook,\n",
    "    report=report,\n",
    "    sheet_name = \"randomforest10_report\"\n",
    ")\n",
    "workbook.save(\"randomforest10_report.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
