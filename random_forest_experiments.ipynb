{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f2c0941",
   "metadata": {},
   "source": [
    "This notebook file contains all the code used for the experiments that were done on random forest. Data and results can be found in the offically used data folder. This file is a cleaned up version of the file the experiments were run in. I kept that file accesible in the legacy code folder as \"randomforest_legacy.ipynb\", but this file contains all the useful code with none of the errant test functions and print statements. I also added more comments to this file for readability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b339e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyrosm.data import sources\n",
    "import pyrosm\n",
    "from collections import Counter, defaultdict\n",
    "import json\n",
    "import pandas as pd\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "import pickle\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# For below import to work move convert_report2excel from support files to the same directory as this file\n",
    "from convert_report2excel import convert_report2excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2e6373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The functions below are several helper functions to vectorize our data\n",
    "\n",
    "def create_tag_lists(pois, n: int) -> tuple[list[str], dict[str:int]]:\n",
    "    \"\"\"\n",
    "    Returns all_good_tags, a list of the tags that appear more than n\n",
    "    Returns tag2idx, a dictionary linking every allowed tag to an index number\n",
    "    \"\"\"\n",
    "\n",
    "    # This function is also used and introduced in our data notebook\n",
    "    # In this notebook we don't use the full functionality, we just need the tag2idx for our vectors\n",
    "    # For a better breakdown of this function, check the data notebook\n",
    "\n",
    "    tag_freq = defaultdict(int)\n",
    "    \n",
    "    for idx, row in pois.iterrows(): \n",
    "        tags = row.get(\"tags\", {})\n",
    "        \n",
    "        if isinstance(tags, dict) and tags:\n",
    "            for tag_key in tags:\n",
    "                tag_freq[tag_key] += 1\n",
    "\n",
    "    all_tags = list(tag_freq.keys())\n",
    "    all_good_tags = [tag for tag in tag_freq if tag_freq[tag] > n]\n",
    "    tag2idx = {tag: i for i, tag in enumerate(all_good_tags)}\n",
    "    idx2tag = {i: tag for tag, i in tag2idx.items()}\n",
    "\n",
    "    tag_freq = dict(sorted(tag_freq.items(), key = lambda x: x[1], reverse = True))\n",
    "    print(f\"All tags sorted by frequency: {tag_freq}\")\n",
    "    print(f\" All allowed tags: {all_good_tags}\")\n",
    "    print(f\"Len all tags: {len(all_tags)}, Len good tags: {len(all_good_tags)}\")\n",
    "\n",
    "    # We only return these two objects\n",
    "    # During testing and in other versions I have returned tag_freq or idx2tag but that wasn't needed for the experiments on random forest\n",
    "    return (all_good_tags, tag2idx)    \n",
    "\n",
    "def tags_to_vec(tag_dict, tag2idx):\n",
    "    \"\"\"\n",
    "    Function that creates a vector from an object\n",
    "    The vector has 0s for the tags it doesnt have, 1s for the tags it does have\n",
    "    \"\"\"\n",
    "    vector = np.zeros(len(tag2idx), dtype= np.float32)\n",
    "    if isinstance(tag_dict, dict) or isinstance(tag_dict, list):\n",
    "        for tag in tag_dict:\n",
    "            if tag in tag2idx:\n",
    "                vector[tag2idx[tag]] = 1\n",
    "\n",
    "\n",
    "    # The below code snippet was not used in the experiments, I just realised while commenting this file that this would be easier than having two functions\n",
    "    # Instead I used the tags_to_vec_singular\n",
    "    elif isinstance(tag_dict, str):\n",
    "        if tag_dict in tag2idx:\n",
    "            vector[tag2idx[tag_dict]] = 1\n",
    "\n",
    "    return vector\n",
    "\n",
    "def tags_to_vec_singular(solo_tag, tag2idx):\n",
    "    \"\"\"\n",
    "    Function that creates a vector from a single tag\n",
    "    Has a single 1 for the tag\n",
    "    This was made to vectorize the y_test set\n",
    "    \"\"\"\n",
    "\n",
    "    # During my experiments I made this after finalizing how I would pass my data to Random Forest\n",
    "    # And because I was passing single tags as part of y_test I couldn't use the standard tags_to_vec above\n",
    "    # And instead of being smart and adding the code snippet above I created a whole new function\n",
    "\n",
    "    vector = np.zeros(len(tag2idx), dtype= np.float32)\n",
    "    if solo_tag in tag2idx:\n",
    "        vector[tag2idx[solo_tag]] = 1\n",
    "    return vector\n",
    "\n",
    "def vector_pois(pois, tag2idx, n):\n",
    "    \"\"\"\n",
    "    Function that turns data in the form of pyrosm pois into vectors with the hel pof a tag2idx object and tag2vec helper functions\n",
    "    n is the length an object has to be to get vectorized\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for tag in pois[\"tags\"]:\n",
    "        if not isinstance(tag, dict) or len(tag) < n: # set the len to 1 will include empty {}\n",
    "        # Even though the length of the vectors shoudl have been set during data, I still kept this part as it could not hurt\n",
    "            continue\n",
    "        \n",
    "        # We shuffle the tags of the object then choose the middle one\n",
    "        # This tag gets taken out of the object and becomes part of the Y set\n",
    "        tag_keys = list(tag.keys())\n",
    "        np.random.shuffle(tag_keys)\n",
    "        feature_idx = len(tag_keys) // 2\n",
    "\n",
    "        # Here we put them together\n",
    "        input_tags = {k: tag[k] for k in tag_keys[:feature_idx] + tag_keys[feature_idx + 1:]}\n",
    "        output_tags = {tag_keys[feature_idx] : tag[tag_keys[feature_idx]]}\n",
    "\n",
    "        # Vectorizing the tags together \n",
    "        X.append(tags_to_vec(input_tags, tag2idx))\n",
    "        y.append(tags_to_vec(output_tags, tag2idx))\n",
    "\n",
    "    X = np.stack(X)\n",
    "    y = np.stack(y)\n",
    "    \n",
    "    print(\"X shape:\", X.shape)\n",
    "\n",
    "    print(\"y_shape\", y.shape)\n",
    "\n",
    "    return X,y\n",
    "\n",
    "def vector_pois_test(poisX, poisy, tag2idx, n):\n",
    "    \"\"\"\n",
    "    Function that turns data in the form of pyrosm pois into vectors with the hel pof a tag2idx object and tag2vec helper functions\n",
    "    n is the length an object has to be to get vectorized\n",
    "    \"\"\"\n",
    "    \n",
    "    # Once again this was an extra function made after the data format was made\n",
    "    # This is a function very similar to the normal vector_pois function, but this one has tags_to_vec_singular\n",
    "    # And this one could have been avoided as well as the change proposed in tags_to_vec had arrived into my brain sooner\n",
    "\n",
    "    # This function is so similar im tempted to take it out but as we used it in the experiments i will keep it in\n",
    "    # For comments, check the function above\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for tag in poisX:\n",
    "        \n",
    "        if len(tag) < n: # set the len to 1 will include empty {}\n",
    "            print(tag)\n",
    "            continue\n",
    "        X.append(tags_to_vec(tag, tag2idx))\n",
    "    for tag in poisy:\n",
    "        y.append(tags_to_vec_singular(tag, tag2idx))\n",
    "\n",
    "    \n",
    "    X = np.stack(X)\n",
    "    y = np.stack(y)\n",
    "    print(\"X_shape:\", X.shape)\n",
    "\n",
    "    print(\"y_shape\", y.shape)\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3201ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the training set to create the tree\n",
    "with open('trainingset', 'rb') as fp:\n",
    "    pois = pickle.load(fp)\n",
    "# And the test set for querying and answers for checking\n",
    "with open('testset_questions', 'rb') as fp:\n",
    "    test_pois_X = pickle.load(fp)\n",
    "with open('testset_answer', 'rb') as fp:\n",
    "    test_pois_y = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06c5a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data should be all cleaned up so we can just leave these at 1\n",
    "n_good_tags = 1 # Frequency of tags for them to be allowed\n",
    "n_per_instance = 1 # Amount of tags an instance needs to have to be part of the data\n",
    "                   # If set to 1 it will include empty cells\n",
    "\n",
    "# As explained, this function is called just to get the tag2idx\n",
    "_, tag2idx = create_tag_lists(pois, n_good_tags)\n",
    "\n",
    "# Creating our X and y sets, these are sets of vectors\n",
    "X, Y = vector_pois(pois, tag2idx, n_per_instance)\n",
    "X_test, y_test = vector_pois_test(test_pois_X, test_pois_y, tag2idx, 1)\n",
    "\n",
    "# Split off part of our training set to become validation set for adaptive thresholding\n",
    "X_train, y_train, X_val, y_val = iterative_train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656aca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true to tune parameters\n",
    "check_model = False\n",
    "if check_model:\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "\n",
    "    # Define search space\n",
    "    param_dist = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=20,  # Try 20 random combinations\n",
    "        cv=3,       # 3-fold cross-validation\n",
    "        scoring='f1_weighted',  \n",
    "        n_jobs=-1,  \n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model after search\n",
    "    best_model = search.best_estimator_\n",
    "    print(best_model)\n",
    "\n",
    "# If set to false we use this, which was our previous best model\n",
    "if not check_model:\n",
    "    best_model = RandomForestClassifier(max_features='log2', n_estimators=300, max_depth=None) # Thepreviousbestmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f902630f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451b1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to find the best thresholds for our adaptive thresholding\n",
    "# Utilizes the validation set we made earlier\n",
    "def find_best_thresholds(y_true, y_probs, thresholds=np.arange(0.1, 0.9, 9)):\n",
    "    best_thresholds = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        best_f1 = 0\n",
    "        best_thresh = 0.5\n",
    "        for thresh in thresholds:\n",
    "            preds = (y_probs[:, i] >= thresh).astype(int)\n",
    "            f1 = f1_score(y_true[:, i], preds, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_thresh = thresh\n",
    "        best_thresholds.append(best_thresh)\n",
    "    return np.array(best_thresholds)\n",
    "\n",
    "Y_probs_val = best_model.predict_proba(X_val)      \n",
    "prob_matrix_val = np.array([\n",
    "    probs[:, 1] if probs.shape[1] > 1 else np.zeros(probs.shape[0])\n",
    "    for probs in Y_probs_val]).T\n",
    "\n",
    "thresholds = find_best_thresholds(y_val, prob_matrix_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5c7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the adaptive thresholding to our predictions\n",
    "Y_probs_test = best_model.predict_proba(X_test)\n",
    "\n",
    "prob_matrix_test = np.array([\n",
    "     probs[:, 1] if probs.shape[1] > 1 else np.zeros(probs.shape[0])\n",
    "     for probs in Y_probs_test]).T\n",
    "\n",
    "Y_preds = (prob_matrix_test >= thresholds).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, Y_preds, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3581733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code utilizes convert_report2excel to put the classifaction report into excel for easier lookup\n",
    "\n",
    "workbook = Workbook()\n",
    "workbook.remove(workbook.active) # Delete default sheet.\n",
    "\n",
    "report = classification_report(\n",
    "    y_test,\n",
    "    Y_preds,\n",
    "    zero_division=0,\n",
    "    output_dict=True\n",
    ")\n",
    "\n",
    "workbook = convert_report2excel(\n",
    "    workbook=workbook,\n",
    "    report=report,\n",
    "    sheet_name = \"schematree2_report\"\n",
    ")\n",
    "workbook.save(\"schematree2_report.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
